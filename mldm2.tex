\documentclass[11pt]{article}
\usepackage{a4wide,times}
\usepackage{parskip}
\usepackage{lmodern}
\usepackage{multicol}
\usepackage{url}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{color}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[utf8]{inputenc}
\usepackage{tgpagella}

\numberwithin{equation}{section}

\DeclareMathOperator*{\argmin}{\arg\min}

\begin{document}

\date{}
\title{\huge \bf{Using support vector machines and\\ decision tree-based methods for\\ analysing gene expression data}}\maketitle
\centerline{\Large {Machine Learning and Algorithms for Data Mining --- Assignment 2}}
\vspace{1em}
\centerline{(Word count: 2161)}
\vspace{2em}

\begin{abstract}
\noindent
\textit{This report presents a methodology for classification of gene expression data using support vector machines and various methods based on decision trees (bagging, random forests, adaptive and gradient boosting). The aim is to distinguish patients with rheumatoid arthritis from healthy controls by processing expression levels of 22,283 genes from three clinical groups in Jena, Berlin and Leipzig \cite{woetzel2014}. The first section gives an overview of the clinical problem and presents the theoretical aspects behind the previously mentioned algorithms. Following this, I describe the dataset pre-processing and software packages used to implement my methodology. Finally, the evaluation section describes the results obtained by applying the different methods to the gene expression data.}
\end{abstract}

\section{Introduction}

\subsection{Motivation}

\textit{Rheumatoid arthritis} (RA) is one of the most common forms of arthritis---a progressive, chronically inflammatory and destructive joint disease that is sustained by an invasive synovial membrane. Distinguishing RA patients from those affected by other inflammatory or degenerative joint diseases or healthy controls, based solely on differentially expressed genes, is difficult. Consequently, we require other unbiased approaches for classification; one such method is discussed by Woetzel et al. \cite{woetzel2014}, who employed rule-based classifiers to discriminate between RA, osteoarthritis (OA) and healthy controls. 

Using a subset of the data described in their research, my work addresses binary classification, distinguishing RA patients from the control group. The machine learning methods that I applied---support vector machines and decision tree-based methods---are described in the following subsections.

\subsection{Support vector machines}

The \textit{support vector machine} (SVM) represents a supervised machine learning technique that performs binary classification by separating the classes with a \textit{hyperplane} in the feature space. For optimal performance, an SVM aims to find the biggest distance from the plane to the boundary data points.

A $p$-dimensional hyperplane is defined as:
\begin{equation}
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0,
\end{equation}

such that if the data point $\mathbf{X} = (X_1, X_2, ..., X_p)^\top$ satisfies the equation, then it lies on the hyperplane.

Assuming the samples in our dataset take the form $(\mathbf{X}_i, Y_i)$, with $Y_i \in \{-1, +1\}$ (for binary classification), we aim to learn a function $f(\mathbf{X}) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$ that approximates the response variable $Y$ such that examples from one class lie on one side of the hyperplane ($f(\mathbf{X}) < 0$) and the others satisfy $f(\mathbf{X}) > 0$.

Then, we need to select the hyperplane that achieves the \textit{maximal margin} between the two classes, defined as the biggest perpendicular distance between the plane and the points closest to it.

This can be defined as a \textit{constraint optimisation problem}---find a set of values $\beta_i, i = 0, 1, 2, ..., p$ to maximise $M$, subject to $\sum_{i = 0}^{p} \beta_i^2 = 1$, such that:
\begin{equation}
\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + ... + \beta_pX_{ip} \geq M, \forall i = 1, 2, ..., n,
\end{equation}

where $n$ is the number of training examples.

However, deriving the best hyperplane might result in overfitting and poorer generalisation ability of the SVM when exposed to unseen data. Instead, we can have a better performance on the test set by misclassifying a few training examples:
\begin{itemize}
\item introduce and find values for \textit{slack variables} $\epsilon_1, \epsilon_2, ..., \epsilon_n$ representing how far away each data point can be on the wrong side of the hyperplane;
\item re-design the constraint optimisation problem such that:
\begin{equation}
\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + ... + \beta_pX_{ip} \geq M(1-\epsilon_i), \forall i = 1, 2, ..., n,
\end{equation}
with $\epsilon_i \geq 0$ and $\sum_{i = 1}^{n} \epsilon_i \leq C$.
\end{itemize}

Problems might still arise when we cannot linearly separate two classes. In this case, the solution is to \textit{enlarge the feature space} through various functions of features. \textit{Kernel functions} can quantify how similar two data points are, while avoiding complexities such as computing polynomials in feature space (which quickly becomes intractable). The kernels I used are:
\begin{enumerate}
\item linear kernel (corresponds to the support vector classifier):
\begin{equation}
K(x_i, x_{i'}) = \sum_{j = 1}^{p} x_{ij} x_{i'j},
\end{equation}
\item polynomial kernel:
\begin{equation}
K(x_i, x_{i'}) = \bigg(1 + \sum_{j = 1}^{p} x_{ij} x_{i'j}\bigg)^d,
\end{equation}
\item radial (Gaussian) kernel (also known as RBF kernel):
\begin{equation}
K(x_i, x_{i'}) = \exp\bigg(-\gamma\sum_{j = 1}^{p}(x_{ij} - x_{i'j})^2\bigg), \text{with } \gamma > 0.
\end{equation}
\end{enumerate}

\subsection{Decision trees}

Widely used for discriminative tasks, a decision tree learns a hierarchy of \textit{if/else} questions which eventually lead to the classification of the given data point. Nodes in the tree can either represent \textit{questions} or \textit{decisions}---in the latter case, the node is a leaf---thereby providing a series of binary splits in the feature plane. These splits produce non-overlapping regions that correspond to classes.

Overfitting usually occurs when the decision tree represents a highly complex model of the training data, such that all its leaves are \textit{pure} (the tree obtains 100\% accuracy on the training set). This can be solved by either:
\begin{itemize}
\item early stopping of the tree construction (\textit{pre-pruning})---can be done by limiting the maximum depth or number of leaves of the tree;
\item removing/collapsing nodes which contain little information (\textit{pruning}).
\end{itemize}

At each step, we choose the variable that best splits the set of items according to an \textit{impurity} measure; the one used in this study is the Gini measure:
\begin{equation}
H(N_m) = \sum_{k = 1}^{C} p_{mk} (1 - p_{mk}),
\end{equation}

where $p_{mk}$ is the proportion of examples belonging to class $k$ at node $m$. The algorithm I used for decision trees is CART (Classification and Regression Trees), which constructs binary trees using the feature and threshold that give the largest information gain upon every split.

\subsection{Bagging}

Decision trees are easy to construct, but they can produce noisy or weak classifiers. More powerful machine learning models can be obtained by \textit{ensembling} multiple classifiers. One of these approaches is called \textit{bagging} and aims to improve prediction performance by reducing the variance of the classifier and obtaining smoother decision boundaries. The bagging algorithm proceeds in the following manner:
\begin{itemize}
\item draw $B$ bootstrap samples $\mathbf{S'}_i, i = 1, 2, ..., B$ with replacement from the dataset $\mathbf{S}$;
\item use each sample $\mathbf{S'}_i$ to train a separate decision tree and record its out-of-bag (OOB) error rate on the examples from $\mathbf{S}$ that were not included in $\mathbf{S'}_i$;
\item average the recorded values and provide a final estimate for the OOB error.
\end{itemize}

The classification performance is improved, as recording $B$ values for the OOB error reduces its variance from an initial value of $\sigma^2$ to $\frac{\sigma^2}{B}$.

\subsection{Random forests}

A further refinement to decision trees, \textit{random forests} (RFs) improve on the bagging approach by de-correlating the trees. This is crucial when we encounter a strong predictor variable in the dataset; most trees will first split the data by this predictor, so all of the bagged trees will be similar and their predictions highly correlated. This in turn will hurt the variance, since averaging many highly correlated quantities does not reduce the variance as much as if the quantities are uncorrelated.

RFs solve this issue by considering only a random subset of $m$ of the $p$ predictors at each split in the tree. The values I used for $m$ were $m = \sqrt{p}$ and $m = \log_2p$.

\subsection{Boosting}

\textit{Boosting} approaches improve on random forests by averaging trees which are grown to weighted versions of the training set. I use two types of boosting:
\begin{itemize}
\item gradient boosting, which takes three hyperparameters ($B$---the number of steps, $\epsilon$---the shrinkage factor, $d$---the maximum tree depth) and proceeds as follows: starting from a model $\hat{G} = 0$ and a residual vector $\mathbf{r} = \mathbf{y}$, at each step we fit a regression tree $\tilde{g}_i$ to the data, then we update the model $\hat{G}_i = \hat{G}_{i-1} + \epsilon \cdot \tilde{g}_i$ and the residuals $r_i = r_i - (\epsilon \cdot \tilde{g}_i)(\mathbf{X}_i)$;
\item adaptive boosting (AdaBoost): a special case of gradient boosting that assigns higher importance to more poorly fitted training examples.
\end{itemize}

\section{Implementation}

All the work presented in this report has been carried out in the Python programming language, using various libraries that I will describe in the following subsections.

\subsection{Processing the dataset}

The data I used to evaluate the various machine learning models was downloaded from the following GEO DataSet browser locations and represents three clinical groups in Jena, Berlin and Leipzig:

\begin{verbatim}
https://www.ncbi.nlm.nih.gov/sites/GDSbrowser?acc=GDS5401
https://www.ncbi.nlm.nih.gov/sites/GDSbrowser?acc=GDS5402
https://www.ncbi.nlm.nih.gov/sites/GDSbrowser?acc=GDS5403
\end{verbatim}

For the purpose of the report methodology, I only extracted the columns representing healthy controls (30 in total) and rheumatoid arthritis patients (33 across the three datasets) for binary classification. The \texttt{GEOparse} library \cite{geoparse}, based on the R library \texttt{GEOquery}, allowed me to parse the data from the SOFT files in a suitable format---a table which facilitated the retrieval of data for the mentioned patients, as they are indexed by columns. First, I loaded the SOFT files into Python:
\begin{verbatim}
gds = GEOparse.get_GEO(filepath="./GDS5401_full.soft")
\end{verbatim}
Next, I prepared containers for the predictor (gene expression levels) and response variables (indicator whether the person is a healthy control or an RA patient):
\begin{verbatim}
Xs = np.empty(shape=(num_samples, num_features), dtype=float)
ys = np.empty(shape=(num_samples,), dtype=int)
\end{verbatim}
To extract data for the healthy controls (C) and rheumatoid arthritis patients (RA), I first looked at the corresponding ranges on the GEO webpages,  under the 'Sample Subsets' tab (e.g. the GDS5403 dataset contains RA patients in the range of identifiers GSM13373\textbf{14}--GSM13373\textbf{26}).

After retrieving the columns through \texttt{cols = gds.table.columns}, I filtered the relevant data from the rest (the ranges [\texttt{l\_C}, ..., \texttt{r\_C}] and [\texttt{l\_RA}, ..., \texttt{r\_RA}] are the ones identified as described above):
\begin{verbatim}
curr_sample = 0
for key in cols:
   # Check if this column corresponds to patient data
    if 'GSM' in key:
        if l_C <= key[-2:] and key[-2:] <= r_C:
            # Class = healthy control
            ys[curr_sample] = 0
        elif l_RA <= key[-2:] and key[-2:] <= r_RA:
            # Class = RA patient
            ys[curr_sample] = 1
        else:
            # Other data - not interested
            continue
        
        # Record gene expression levels for the current person
        Xs[curr_sample, :] = gds.table[key]
        curr_sample += 1
\end{verbatim}

\subsection{Machine learning models}

Classification methodologies require evaluation and comparison; the \texttt{scikit-learn} library \cite{scikit-learn} provides simple interfaces to many ML models, including SVMs, decision trees, random forests and ensembling approaches such as bagging and boosting. Furthermore, the library contains useful evaluation metrics (e.g. confusion matrix, recall, precision, F1 score).

The following lines illustrate how to instantiate all the classifiers I used for this report:
\begin{verbatim}
SVM_linear = svm.SVC(kernel='linear', C=C)
SVM_poly = svm.SVC(kernel='poly', C=C, degree=deg)
SVM_rbf = svm.SVC(kernel='rbf', C=C, gamma=gamma)

dt = tree.DecisionTreeClassifier(max_depth=max_depth)

b = BaggingClassifier(base_estimator=None, # defaults to a decision tree
                      n_estimators=num_trees,
                      oob_score=True)
                      
rf = RandomForestClassifier(n_estimators=num_trees,
                            max_features=max_features, # log2 or sqrt
                            oob_score=True)

gb = GradientBoostingClassifier(n_estimators=num_trees,
                                max_depth=max_depth)

adab = AdaBoostClassifier(n_estimators=num_trees)
\end{verbatim}

The performance of SVMs, decision trees, gradient boosting and adaptive boosting was evaluated using the metrics below and saved to pickled files (the \texttt{pickle} module performs Python object serialization):
\begin{verbatim}
cm = confusion_matrix(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Save the results in a pickled format
res = { "cm" : cm, "recall" : recall, "precision" : precision, "f1" : f1 }
pickle.dump(res, open(filename, "wb"))
\end{verbatim}

Bagging and random forests perform bootstrap sampling with replacement, so their evaluation consists of out-of-bag error estimates:
\begin{verbatim}
oob_error = 1 - b.oob_score_
oob_error = 1 - rf.oob_score_
\end{verbatim}

\section{Evaluation}

All machine learning models described in this report were evaluated with 20 random train/test partitions of the dataset and the results were averaged, in order to provide a better indicator of the real accuracy. As the dataset is quite small---63 examples in total---a single model is likely to encounter high variance on other data, so the repeated random partitioning helps reduce this variance.

\subsection{SVMs}

To evaluate the performance of SVM models, I trained and tested one seperate model for every combination of the following parameters:
\begin{itemize}
\item for all SVMs: $C = \{0.1, 1, 10\}$;
\item polynomial kernel SVMs: $\textit{degree} = \{2, 3, ..., 15\}$;
\item RBF kernel SVMs: $\gamma = \{0.1, 1, 10\}$.
\end{itemize}

Table \ref{table:resSVM} describes the best results for each type of SVM (chosen by the highest value for the F1 score, which incorporates both precision and recall), along with the corresponding hyperparameters.
\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
  & Precision & Recall & F1 \\ 
 \hline
 Linear kernel ($C = 0.1$) & \bf{0.92} & \bf{1.0} & \bf{0.96} \\ 
 \hline
 Polynomial kernel ($C = 0.1, d = 2$) & 0.91 & 0.97 & 0.93 \\
 \hline
 RBF kernel ($C = 0.1, \gamma = 0.1$) & 0.64 & 1.0 & 0.77 \\ 
 \hline
\end{tabular}
\caption{Results for the best \textbf{SVM} models.}
\label{table:resSVM}
\end{center}
\end{table}

It can be noticed that a linear boundary separates the two classes well enough, while doing feature expansion through a second-degree polynomial kernel already reduces the performance of the SVM. With an F1 score of 0.77, the Gaussian kernel is clearly unsuitable for discriminating between RA and healthy controls, which is in agreement with the linear kernel performing so well (the Gaussian kernel is not likely to cover the area outside the one determined by training examples).

\subsection{Decision tree-based methods}

A single \textbf{decision tree} was evaluated with no bounds on the maximum depth, and then limited by $depth = \{1, 2, 4, 7\}$. As seen in Table \ref{table:resDT}, the unbounded tree obtains the best performance (only slightly better than a tree with depth 4). This might be due to numerous splits being required to finally make a decision regarding the class, using all of the 22,283 features.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
  & Precision & Recall & F1 \\ 
 \hline
 Depth = 1 & 0.851 & 0.884 & 0.854 \\
 \hline
 Depth = 2 & 0.859 & 0.897 & 0.866 \\ 
 \hline
 Depth = 4 & 0.894 & 0.924 & 0.896 \\ 
 \hline
 Depth = 7 & 0.874 & 0.884 & 0.866 \\ 
 \hline
 Depth = $\infty$ (unbounded) & \bf{0.879} & \bf{0.938} & \bf{0.903} \\ 
 \hline
\end{tabular}
\caption{Results for a single \textbf{decision tree}.}
\label{table:resDT}
\end{center}
\end{table}

\textbf{Bagging} is expected to improve on the performance of a single decision tree; here, only the OOB error estimate is available. Figure 1 shows that it tends to oscillate between $0.6$ and $0.9$ for most values of $t$. However, the minimum value for the OOB error is $0.038$ (so a proportion of 96.2\% correctly classified out-of-bag examples), at $t = 33$ trees.

\begin{figure}[H]
\includegraphics[width=\textwidth]{bagging.png}
\caption{Out-of-bag (OOB) error rates for the \textbf{bagging} method.}
\end{figure}

Even further improvements (once again, expected) can be noted from the results of evaluating \textbf{random forests}. This is an effect of limiting the number of features considered at each split, such that the predictions of the obtained trees are decorrelated. Figure 2 indicates that when we allow only $m = \sqrt{p}$ and $m = \log_2p$ features upon splitting, the minimum values for the OOB error are both equal to $0.0$, at $t = 79$ trees and $t = 64$ trees, respectively. However, this estimate might change if the dataset used for evaluation is considerably larger, but it still indicates a significant improvement over bagging. The graph also confirms the fact that, once a sufficient number of trees is reached, the performance cannot degrade.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rf.png}
\caption{Out-of-bag (OOB) error rates for \textbf{random forests}, shown in \textcolor{blue}{blue} for $m = \sqrt{p}$ and \textcolor{green}{green} for $m = \log_2p$.}
\end{figure}

\textbf{Gradient boosting} was evaluated with the following hyperparameters: maximum depth of the trees $d$ between 2 and 10, number of trees $t$ ranging from 10 to 100. The best performance (chosen by the largest F1 score) is achieved by ($d = 3$, $t = 25$):
\begin{itemize}
\item precision: 0.93
\item recall: 0.95
\item \underline{F1: 0.93}.
\end{itemize}

I evaluated the \textbf{adaptive boosting} approach by running a separate model for any number of trees between $10$ and $200$. As seen in Figure 3, the \underline{F1 metric peaks at $0.91$}, for $t = 74$ trees.

\begin{figure}[H]
\includegraphics[width=\textwidth]{adaboost.png}
\caption{F1 score as a function of the number of trees for \textbf{adaptive boosting}.}
\end{figure}

Although boosting methods usually produce more powerful discriminators than random forests, the size of the dataset might have affected the outcome in this study. Both gradient and adaptive boosting still perform better than a single decision tree---which is highly expected---but they fail to reach the (obviously superior) performance of random forests.

\section{Conclusion}

The machine learning algorithms discussed in this report---especially the decision tree-based ones---generally behaved as initially discussed from a theoretical perspective, when being exposed to RA/healthy controls data. The best performances were obtained by the linear SVM model (F1 score of 0.96) and the random forest approach (OOB score of 0.0), but using significantly larger datasets to train the classifiers might produce different results which are likely to indicate performances closer to the real ones.

In the case of SVMs, the superiority of the linear kernel from the polynomial and RBF ones indicates that a separating hyperplane provides a good approximation to the boundary between the two classes. Bagging was clearly outperformed by random forests, while decision trees were also surpassed by boosting approaches (F1 scores of 0.90, 0.91 and 0.93, respectively). Nevertheless, these results show an impressive performance for a dataset of 63 examples, illustrating the power of the various improvements that are added to a basic algorithm (e.g. taking bootstrap samples of the dataset to reduce variance and create smoother decision boundaries).

\newpage
\medskip
\begin{thebibliography}{9}
\bibitem{woetzel2014}
Woetzel, D., Huber, R., Kupfer, P., Pohlers, D., Pfaff, M., Driesch, D., ... \& Kinne, R. W. (2014). Identification of rheumatoid arthritis and osteoarthritis patients by transcriptome-based rule set generation. Arthritis research \& therapy, 16(2), R84.
\bibitem{mldm}
Jamnik, M., Li\`o, P., \& Sauerwald, T. (2017). Lecture notes for \textit{Machine Learning and Algorithms for Data Mining} (University of Cambridge Computer Laboratory - MPhil in Advanced Computer Science).
\bibitem{geoparse}
Gumienny, R. (2015). GEOparse: Python library to access Gene Expression Omnibus Database (GEO). \texttt{https://geoparse.readthedocs.io/en/latest/introduction.html}.
\bibitem{scikit-learn}
Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825--2830.

\end{thebibliography}

\end{document}
